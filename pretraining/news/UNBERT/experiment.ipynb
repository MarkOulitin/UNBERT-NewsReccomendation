{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import MindDataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news info from data\\small\\news_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "train_set = MindDataset(\n",
    "    root='data',\n",
    "    tokenizer=tokenizer,\n",
    "    mode='train',\n",
    "    split='small',\n",
    "    news_max_len=20,\n",
    "    hist_max_len=20,\n",
    "    seq_max_len=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news info from data\\small\\news_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "val_set = MindDataset(\n",
    "    root='data',\n",
    "    tokenizer=tokenizer,\n",
    "    mode='dev',\n",
    "    split='small',\n",
    "    news_max_len=20,\n",
    "    hist_max_len=20,\n",
    "    seq_max_len=300\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run Model on Small"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "from sklearn.metrics import roc_auc_score, log_loss, accuracy_score\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from time import gmtime, strftime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from src.data_loader import MindDataset\n",
    "from src.model import UNBERT\n",
    "from src.eval import dev, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--mode', type=str, default='train')\n",
    "    parser.add_argument('--root', type=str, default='data')\n",
    "    parser.add_argument('--split', type=str, default='small')\n",
    "    parser.add_argument('--pretrain', type=str, default='bert-base-uncased')\n",
    "    parser.add_argument('--level_state', type=str, default='word',\n",
    "                        help='word, news or both')\n",
    "    parser.add_argument('--news_mode', type=str, default='nseg',\n",
    "                        help='nseg, mean or attention')\n",
    "    parser.add_argument('--news_max_len', type=int, default=20)\n",
    "    parser.add_argument('--hist_max_len', type=int, default=20)\n",
    "    parser.add_argument('--seq_max_len', type=int, default=300)\n",
    "    parser.add_argument('--restore', type=str, default=None)\n",
    "    parser.add_argument('--output', type=str, default='./output')\n",
    "    parser.add_argument('--epoch', type=int, default=5) # set 5 in small dataset, 2 in large\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--lr', type=float, default=2e-5)\n",
    "    parser.add_argument('--eval_every', type=int, default=10000)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--mode MODE] [--root ROOT] [--split SPLIT]\n",
      "                             [--pretrain PRETRAIN] [--level_state LEVEL_STATE]\n",
      "                             [--news_mode NEWS_MODE]\n",
      "                             [--news_max_len NEWS_MAX_LEN]\n",
      "                             [--hist_max_len HIST_MAX_LEN]\n",
      "                             [--seq_max_len SEQ_MAX_LEN] [--restore RESTORE]\n",
      "                             [--output OUTPUT] [--epoch EPOCH]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--eval_every EVAL_EVERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\noysa\\AppData\\Roaming\\jupyter\\runtime\\kernel-57432274-bfe6-45d9-b685-e92bb4f1b273.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "args = parse_args()\n",
    "log_file = os.path.join(args.output, \"{}-{}-{}.log\".format(\n",
    "                args.mode, args.split, strftime('%Y%m%d%H%M%S', gmtime())))\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "def printzzz(log):\n",
    "    with open(log_file, \"a\") as fout:\n",
    "        fout.write(log + \"\\n\")\n",
    "    print(log)\n",
    "\n",
    "printzzz(str(args))\n",
    "model = UNBERT(pretrained=args.pretrain,\n",
    "               level_sate=args.level_state,\n",
    "               news_mode=args.news_mode,\n",
    "               max_len=args.seq_max_len)\n",
    "if args.restore is not None and os.path.isfile(args.restore):\n",
    "    printzzz(\"restore model from {}\".format(args.restore))\n",
    "    state_dict = torch.load(args.restore, map_location=torch.device('cpu'))\n",
    "    st = {}\n",
    "    for k in state_dict:\n",
    "        if k.startswith('bert'):\n",
    "            st['_model'+k[len('bert'):]] = state_dict[k]\n",
    "        elif k.startswith('classifier'):\n",
    "            st['_dense'+k[len('classifier'):]] = state_dict[k]\n",
    "        else:\n",
    "            st[k] = state_dict[k]\n",
    "    model.load_state_dict(st)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
    "if args.mode == \"train\":\n",
    "    printzzz('reading training data...')\n",
    "    train_set = MindDataset(\n",
    "        args.root,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='train',\n",
    "        split=args.split,\n",
    "        news_max_len=args.news_max_len,\n",
    "        hist_max_len=args.hist_max_len,\n",
    "        seq_max_len=args.seq_max_len\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8\n",
    "    )\n",
    "    printzzz('reading dev data...')\n",
    "    dev_set = MindDataset(\n",
    "        args.root,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='dev',\n",
    "        split=args.split,\n",
    "        news_max_len=args.news_max_len,\n",
    "        hist_max_len=args.hist_max_len,\n",
    "        seq_max_len=args.seq_max_len\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=dev_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    m_optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "    m_scheduler = get_linear_schedule_with_warmup(m_optim,\n",
    "                num_warmup_steps=len(train_set)//args.batch_size*2,\n",
    "                num_training_steps=len(train_set)*args.epoch//args.batch_size)\n",
    "    loss_fn.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = nn.DataParallel(loss_fn)\n",
    "    printzzz(\"start training...\")\n",
    "\n",
    "    best_auc = 0.0\n",
    "    for epoch in range(args.epoch):\n",
    "        avg_loss = 0.0\n",
    "        batch_iterator = tqdm(train_loader, disable=False)\n",
    "        for step, train_batch in enumerate(batch_iterator):\n",
    "            batch_score = model(train_batch['input_ids'].to(device),\n",
    "                                train_batch['input_mask'].to(device),\n",
    "                                train_batch['segment_ids'].to(device),\n",
    "                                train_batch['news_segment_ids'].to(device),\n",
    "                                train_batch['category_segment_ids'].to(device),\n",
    "                                train_batch['sentence_ids'].to(device),\n",
    "                                train_batch['sentence_mask'].to(device),\n",
    "                                train_batch['sentence_segment_ids'].to(device),\n",
    "                                )\n",
    "            batch_loss = loss_fn(batch_score, train_batch['label'].to(device))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                batch_loss = batch_loss.mean()\n",
    "            avg_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            m_optim.step()\n",
    "            m_scheduler.step()\n",
    "            m_optim.zero_grad()\n",
    "        auc = dev(model, dev_loader, device, args.output, is_epoch=True)\n",
    "        printzzz(\"Epoch {}, AUC: {:.4f}\".format(epoch+1, auc))\n",
    "        final_path = os.path.join(args.output, \"epoch_{}.bin\".format(epoch+1))\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            torch.save(model.module.state_dict(), final_path)\n",
    "        else:\n",
    "            torch.save(model.state_dict(), final_path)\n",
    "    printzzz(\"train success!\")\n",
    "elif args.mode == \"dev\":\n",
    "    printzzz('reading dev data...')\n",
    "    dev_set = MindDataset(\n",
    "        args.root,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='dev',\n",
    "        split=args.split,\n",
    "        news_max_len=args.news_max_len,\n",
    "        hist_max_len=args.hist_max_len,\n",
    "        seq_max_len=args.seq_max_len\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=dev_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    auc = dev(model, dev_loader, device, args.output, is_epoch=True)\n",
    "    printzzz(\"dev AUC: {:.4f}\".format(auc))\n",
    "    printzzz(\"dev success!\")\n",
    "else:\n",
    "    printzzz('reading test data...')\n",
    "    test_set = MindDataset(\n",
    "        dataset=args.test,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='test',\n",
    "        query_max_len=args.max_query_len,\n",
    "        doc_max_len=args.max_doc_len,\n",
    "        part_tag=args.part_tag\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    test(model, test_loader, device, args.output)\n",
    "    printzzz(\"test success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mSystemExit\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mparse_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m log_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(args\u001B[38;5;241m.\u001B[39moutput, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.log\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m      3\u001B[0m                 args\u001B[38;5;241m.\u001B[39mmode, args\u001B[38;5;241m.\u001B[39msplit, strftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m, gmtime())))\n\u001B[0;32m      4\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(args\u001B[38;5;241m.\u001B[39moutput, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[9], line 20\u001B[0m, in \u001B[0;36mparse_args\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m parser\u001B[38;5;241m.\u001B[39madd_argument(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--lr\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m)\n\u001B[0;32m     19\u001B[0m parser\u001B[38;5;241m.\u001B[39madd_argument(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--eval_every\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m args\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\UNBERT-NewsReccomendation\\lib\\argparse.py:1836\u001B[0m, in \u001B[0;36mArgumentParser.parse_args\u001B[1;34m(self, args, namespace)\u001B[0m\n\u001B[0;32m   1834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m argv:\n\u001B[0;32m   1835\u001B[0m     msg \u001B[38;5;241m=\u001B[39m _(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munrecognized arguments: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m-> 1836\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1837\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m args\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\UNBERT-NewsReccomendation\\lib\\argparse.py:2594\u001B[0m, in \u001B[0;36mArgumentParser.error\u001B[1;34m(self, message)\u001B[0m\n\u001B[0;32m   2592\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_usage(_sys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[0;32m   2593\u001B[0m args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprog\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprog, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m: message}\n\u001B[1;32m-> 2594\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m%(prog)s\u001B[39;49;00m\u001B[38;5;124;43m: error: \u001B[39;49m\u001B[38;5;132;43;01m%(message)s\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\UNBERT-NewsReccomendation\\lib\\argparse.py:2581\u001B[0m, in \u001B[0;36mArgumentParser.exit\u001B[1;34m(self, status, message)\u001B[0m\n\u001B[0;32m   2579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message:\n\u001B[0;32m   2580\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_print_message(message, _sys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[1;32m-> 2581\u001B[0m \u001B[43m_sys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstatus\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mSystemExit\u001B[0m: 2"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "unbert",
   "language": "python",
   "display_name": "UNBERT"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
