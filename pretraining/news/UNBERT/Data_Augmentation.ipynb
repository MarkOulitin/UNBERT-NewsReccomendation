{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7858541,
          "sourceType": "datasetVersion",
          "datasetId": 4609584
        },
        {
          "sourceId": 7877686,
          "sourceType": "datasetVersion",
          "datasetId": 4623240
        },
        {
          "sourceId": 7878014,
          "sourceType": "datasetVersion",
          "datasetId": 4623487
        },
        {
          "sourceId": 7895425,
          "sourceType": "datasetVersion",
          "datasetId": 4636155
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-23T12:33:42.225073Z",
          "iopub.execute_input": "2024-03-23T12:33:42.225344Z",
          "iopub.status.idle": "2024-03-23T12:34:08.895510Z",
          "shell.execute_reply.started": "2024-03-23T12:33:42.225318Z",
          "shell.execute_reply": "2024-03-23T12:34:08.894438Z"
        },
        "trusted": true,
        "id": "flG22e6UNBpL",
        "outputId": "49f24434-8e4c-4b81-a942-7dd67a337c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.38.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.6.0-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.6.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm  # Use auto to select the best interface (notebook, terminal, etc.)\n",
        "import gc\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T12:34:08.897734Z",
          "iopub.execute_input": "2024-03-23T12:34:08.898363Z",
          "iopub.status.idle": "2024-03-23T12:34:30.475888Z",
          "shell.execute_reply.started": "2024-03-23T12:34:08.898327Z",
          "shell.execute_reply": "2024-03-23T12:34:30.475050Z"
        },
        "trusted": true,
        "id": "xSslTv9mNBpM",
        "outputId": "7e8f66c5-ff0b-42c6-f825-599f12351b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-23 12:34:16.953058: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-23 12:34:16.953197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-23 12:34:17.119013: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_process_df(path):\n",
        "    news_mind = pd.read_csv(path, sep = '\\t', header = None)\n",
        "    news_mind.rename(columns = {0:\"news_id\", 1:\"category\", 2:\"sub_category\", 3:\"title\",4:\"abstract\"}, inplace = True)\n",
        "    news_mind.drop(columns = [\"abstract\",\"category\", \"sub_category\",5,6,7], inplace = True)\n",
        "    return news_mind.drop_duplicates()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T12:34:30.476909Z",
          "iopub.execute_input": "2024-03-23T12:34:30.477424Z",
          "iopub.status.idle": "2024-03-23T12:34:30.483570Z",
          "shell.execute_reply.started": "2024-03-23T12:34:30.477398Z",
          "shell.execute_reply": "2024-03-23T12:34:30.482522Z"
        },
        "trusted": true,
        "id": "gVEjcHVqNBpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_path = \"/kaggle/input/mind-small-train/news.tsv\"\n",
        "small_val_path = \"/kaggle/input/mind-small-val/news.tsv\"\n",
        "train_df = read_process_df(small_train_path)\n",
        "print(train_df.shape)\n",
        "val_df = read_process_df(small_val_path)\n",
        "print(val_df.shape)\n",
        "df = pd.concat([train_df, val_df], axis = 0).drop_duplicates().reset_index(drop = True)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T12:34:30.485971Z",
          "iopub.execute_input": "2024-03-23T12:34:30.486355Z",
          "iopub.status.idle": "2024-03-23T12:34:32.814832Z",
          "shell.execute_reply.started": "2024-03-23T12:34:30.486321Z",
          "shell.execute_reply": "2024-03-23T12:34:32.813907Z"
        },
        "trusted": true,
        "id": "xalPkl9ZNBpM",
        "outputId": "eb2b302c-d734-4228-ac72-a662cc4d422e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(51282, 2)\n(42416, 2)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "      news_id                                              title\n0      N55528  The Brands Queen Elizabeth, Prince Charles, an...\n1      N19639                      50 Worst Habits For Belly Fat\n2      N61837  The Cost of Trump's Aid Freeze in the Trenches...\n3      N53526  I Was An NBA Wife. Here's How It Affected My M...\n4      N38324  How to Get Rid of Skin Tags, According to a De...\n...       ...                                                ...\n65233   N2292  House investigators release more impeachment t...\n65234  N27291  Mural in Downtown S.F. Depicts Swedish Teen Cl...\n65235  N52871  Residents of Mexican town struggle with fear a...\n65236  N36658  Apartments for rent in Minneapolis: What will ...\n65237  N39563       Shall we dance: Sports stars shake their leg\n\n[65238 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>N55528</td>\n      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>N19639</td>\n      <td>50 Worst Habits For Belly Fat</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>N61837</td>\n      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>N53526</td>\n      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>N38324</td>\n      <td>How to Get Rid of Skin Tags, According to a De...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65233</th>\n      <td>N2292</td>\n      <td>House investigators release more impeachment t...</td>\n    </tr>\n    <tr>\n      <th>65234</th>\n      <td>N27291</td>\n      <td>Mural in Downtown S.F. Depicts Swedish Teen Cl...</td>\n    </tr>\n    <tr>\n      <th>65235</th>\n      <td>N52871</td>\n      <td>Residents of Mexican town struggle with fear a...</td>\n    </tr>\n    <tr>\n      <th>65236</th>\n      <td>N36658</td>\n      <td>Apartments for rent in Minneapolis: What will ...</td>\n    </tr>\n    <tr>\n      <th>65237</th>\n      <td>N39563</td>\n      <td>Shall we dance: Sports stars shake their leg</td>\n    </tr>\n  </tbody>\n</table>\n<p>65238 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel processing version (utilizes multiple GPUs):**"
      ],
      "metadata": {
        "id": "k3GRJA7IPinr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import gc\n",
        "\n",
        "class BackTranslator:\n",
        "    def __init__(self, batch_size = 8):\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.en_to_fr , self.fr_to_en = None, None\n",
        "        self.en_to_ger , self.ger_to_en = None, None\n",
        "        self.en_to_esp , self.esp_to_en = None, None\n",
        "        self.en_to_arab , self.arab_to_en = None, None\n",
        "        self.en_to_heb , self.heb_to_en = None, None\n",
        "        self.models_cache = {}\n",
        "\n",
        "    def _load_model_and_tokenizer(self, model_name):\n",
        "        if model_name not in self.models_cache:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            self.models_cache[model_name] = (tokenizer, model)\n",
        "        return self.models_cache[model_name]\n",
        "\n",
        "    def transform(self, method, texts):\n",
        "        if method == \"helsinki\":\n",
        "            self.en_to_fr , self.fr_to_en = \"Helsinki-NLP/opus-mt-en-fr\", \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "            self.en_to_ger , self.ger_to_en = \"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\"\n",
        "            self.en_to_esp , self.esp_to_en = \"Helsinki-NLP/opus-mt-en-es\", \"Helsinki-NLP/opus-mt-es-en\"\n",
        "            self.en_to_heb , self.heb_to_en = \"Helsinki-NLP/opus-mt-en-he\", \"Helsinki-NLP/opus-mt-tc-big-he-en\"\n",
        "            paraphrased_batch_french = self._helsinki_back_translation_batch(texts,self.en_to_fr,self.fr_to_en )\n",
        "            paraphrased_batch_german = self._helsinki_back_translation_batch(texts,self.en_to_ger,self.ger_to_en )\n",
        "            paraphrased_batch_esp = self._helsinki_back_translation_batch(texts,self.en_to_esp,self.esp_to_en )\n",
        "            paraphrased_batch_heb = self._helsinki_back_translation_batch(texts,self.en_to_heb,self.heb_to_en)\n",
        "            return paraphrased_batch_french, paraphrased_batch_german, paraphrased_batch_esp, paraphrased_batch_heb\n",
        "        elif method == \"mbart\":\n",
        "            self.en_to_fr , self.fr_to_en = \"en_XX\", \"fr_XX\"\n",
        "            self.en_to_ger , self.ger_to_en = \"en_XX\", \"de_DE\"\n",
        "            self.en_to_esp , self.esp_to_en = \"en_XX\", \"es_XX\"\n",
        "            self.en_to_arab , self.arab_to_en = \"en_XX\", \"ar_AR\"\n",
        "            self.en_to_heb , self.heb_to_en = \"en_XX\", \"he_IL\"\n",
        "            return self._mbart_back_translation(texts)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported back translation method\")\n",
        "\n",
        "    def _helsinki_back_translation_batch(self, texts, src_model_name, tgt_model_name):\n",
        "        # Load tokenizer and model for source to target translation\n",
        "        src_tokenizer, src_model = self._load_model_and_tokenizer(src_model_name)\n",
        "\n",
        "        # Tokenize batch for source language\n",
        "        src_encoded = src_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        # Generate translation in target language\n",
        "        src_translated_tokens = src_model.generate(**src_encoded)\n",
        "        src_translated_texts = src_tokenizer.batch_decode(src_translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Load tokenizer and model for target to source back translation\n",
        "        tgt_tokenizer, tgt_model = self._load_model_and_tokenizer(tgt_model_name)\n",
        "\n",
        "        # Tokenize batch for target language\n",
        "        tgt_encoded = tgt_tokenizer(src_translated_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        # Generate back translation in source language\n",
        "        tgt_translated_tokens = tgt_model.generate(**tgt_encoded)\n",
        "        back_translated_texts = tgt_tokenizer.batch_decode(tgt_translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return back_translated_texts\n",
        "\n",
        "    def process_batch(self,method, batch_texts):\n",
        "            return self.transform(method, batch_texts)\n",
        "\n",
        "    def augment_dataframe_parallel(self, df, df_name, column_name, method, num_workers=4):\n",
        "        texts = df[column_name].tolist()\n",
        "        augmented_texts_french = [None] * len(texts)\n",
        "        augmented_texts_german = [None] * len(texts)\n",
        "        augmented_texts_spanish = [None] * len(texts)\n",
        "        augmented_texts_heb = [None] * len(texts)\n",
        "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "            future_to_batch = {executor.submit(self.process_batch, method, texts[i:i + self.batch_size]): i for i in range(0, len(texts), self.batch_size)}\n",
        "            for future in as_completed(future_to_batch):\n",
        "                batch_index = future_to_batch[future]\n",
        "                try:\n",
        "                    paraphrased_batch_french, paraphrased_batch_german, paraphrased_batch_spanish, paraphrased_batch_heb = future.result()\n",
        "                    augmented_texts_french[batch_index:batch_index + self.batch_size] = paraphrased_batch_french\n",
        "                    augmented_texts_german[batch_index:batch_index + self.batch_size] = paraphrased_batch_german\n",
        "                    augmented_texts_spanish[batch_index:batch_index + self.batch_size] = paraphrased_batch_spanish\n",
        "                    augmented_texts_heb[batch_index:batch_index + self.batch_size] = paraphrased_batch_heb\n",
        "                except Exception as exc:\n",
        "                    print(f\"Batch {batch_index} generated an exception: {exc}\")\n",
        "                finally:\n",
        "                    gc.collect()\n",
        "\n",
        "        df[\"french_augmentation\"] = augmented_texts_french\n",
        "        df[\"german_augmentation\"] = augmented_texts_german\n",
        "        df[\"spanish_augmentation\"] = augmented_texts_spanish\n",
        "        df[\"hebrew_augmentation\"] = augmented_texts_heb\n",
        "        df.to_csv(f\"augmneted_back_translated_{df_name}_{method}.csv\", index = False)\n",
        "        return df"
      ],
      "metadata": {
        "trusted": true,
        "id": "3Pg5FM-6NBpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "indOeWxnNBpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Without parallel processing:**"
      ],
      "metadata": {
        "id": "OegtvBdAPu9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "class BackTranslator:\n",
        "    def __init__(self, batch_size = 8, num_return_sequences = 5):\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.en_to_fr , self.fr_to_en = None, None\n",
        "        self.en_to_ger , self.ger_to_en = None, None\n",
        "        self.en_to_esp , self.esp_to_en = None, None\n",
        "        self.en_to_arab , self.arab_to_en = None, None\n",
        "        self.en_to_heb , self.heb_to_en = None, None\n",
        "        self.num_return_sequences = num_return_sequences\n",
        "        self.models_cache = {}\n",
        "\n",
        "    def _load_model_and_tokenizer(self, model_name):\n",
        "        if model_name not in self.models_cache:\n",
        "            if \"mbart\" in model_name:\n",
        "                print(\"hi\")\n",
        "                tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "                model = MBartForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "            else:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "            self.models_cache[model_name] = (tokenizer, model)\n",
        "        return self.models_cache[model_name]\n",
        "\n",
        "    def transform(self, method, texts):\n",
        "        if method == \"helsinki\":\n",
        "            self.en_to_fr , self.fr_to_en = \"Helsinki-NLP/opus-mt-en-fr\", \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "#             self.en_to_ger , self.ger_to_en = \"Helsinki-NLP/opus-mt-en-de\", \"Helsinki-NLP/opus-mt-de-en\"\n",
        "#             self.en_to_esp , self.esp_to_en = \"Helsinki-NLP/opus-mt-en-es\", \"Helsinki-NLP/opus-mt-es-en\"\n",
        "#             self.en_to_heb , self.heb_to_en = \"Helsinki-NLP/opus-mt-en-he\", \"Helsinki-NLP/opus-mt-tc-big-he-en\"\n",
        "            paraphrased_batch_french = self._helsinki_back_translation_batch(texts,self.en_to_fr,self.fr_to_en )\n",
        "#             paraphrased_batch_german = self._helsinki_back_translation_batch(texts,self.en_to_ger,self.ger_to_en )\n",
        "#             paraphrased_batch_esp = self._helsinki_back_translation_batch(texts,self.en_to_esp,self.esp_to_en )\n",
        "#             paraphrased_batch_heb = self._helsinki_back_translation_batch(texts,self.en_to_heb,self.heb_to_en)\n",
        "            return paraphrased_batch_french\n",
        "        elif method == \"mbart\":\n",
        "            self.en_to_fr , self.fr_to_en = \"en_XX\", \"fr_XX\"\n",
        "            self.en_to_ger , self.ger_to_en = \"en_XX\", \"de_DE\"\n",
        "            self.en_to_esp , self.esp_to_en = \"en_XX\", \"es_XX\"\n",
        "            self.en_to_arab , self.arab_to_en = \"en_XX\", \"ar_AR\"\n",
        "            self.en_to_heb , self.heb_to_en = \"en_XX\", \"he_IL\"\n",
        "            return self._mbart_back_translation(texts, \"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported back translation method\")\n",
        "\n",
        "    def _helsinki_back_translation_batch(self, texts, src_model_name, tgt_model_name):\n",
        "        # Load tokenizer and model for source to target translation\n",
        "        src_tokenizer, src_model = self._load_model_and_tokenizer(src_model_name)\n",
        "\n",
        "        # Tokenize batch for source language\n",
        "        src_encoded = src_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        # Generate translation in target language\n",
        "        src_translated_tokens = src_model.generate(**src_encoded)\n",
        "        src_translated_texts = src_tokenizer.batch_decode(src_translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Load tokenizer and model for target to source back translation\n",
        "        tgt_tokenizer, tgt_model = self._load_model_and_tokenizer(tgt_model_name)\n",
        "\n",
        "        # Tokenize batch for target language\n",
        "        tgt_encoded = tgt_tokenizer(src_translated_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        # Generate back translation in source language\n",
        "        tgt_translated_tokens = tgt_model.generate(**tgt_encoded)\n",
        "        back_translated_texts = tgt_tokenizer.batch_decode(tgt_translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        return back_translated_texts\n",
        "\n",
        "\n",
        "    def _mbart_back_translation(self, texts, model_name):\n",
        "        tokenizer, model = self._load_model_and_tokenizer(model_name)\n",
        "\n",
        "        src_lang = \"en_XX\"\n",
        "#         tgt_langs = [\"fr_XX\", \"de_DE\", \"es_XX\", \"ar_AR\", \"he_IL\"]\n",
        "        tgt_lang = \"fr_XX\"\n",
        "#         augmented_texts = []\n",
        "#         for tgt_lang in tgt_langs:\n",
        "        tokenizer.src_lang = src_lang\n",
        "        encoded_en = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n",
        "                                   num_return_sequences=5,\n",
        "                                   num_beams=5,\n",
        "                                   max_length=512)\n",
        "        translated_to_target = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        tokenizer.src_lang = tgt_lang\n",
        "        encoded_target = tokenizer(translated_to_target, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "        generated_tokens_back = model.generate(**encoded_target, forced_bos_token_id=tokenizer.lang_code_to_id[src_lang],\n",
        "                                   num_return_sequences=5,\n",
        "                                   num_beams=5,\n",
        "                                   max_length=512)\n",
        "        back_translated_to_english = tokenizer.batch_decode(generated_tokens_back, skip_special_tokens=True)\n",
        "#         augmented_texts.append(back_translated_to_english)\n",
        "\n",
        "        augmentations_per_txt = [back_translated_to_english[i:i + self.num_return_sequences]\n",
        "                                 for i in range(0, len(back_translated_to_english), self.num_return_sequences)]\n",
        "        # a transpose operation:\n",
        "        augmentations_per_column = [list(group) for group in zip(*augmentations_per_txt)]\n",
        "        return augmentations_per_column\n",
        "\n",
        "    def augment_dataframe(self, df, df_name, column_name, method):\n",
        "        texts = df[column_name].tolist()\n",
        "        gc_indicator = 0\n",
        "        for j in range (self.num_return_sequences):\n",
        "            df[f\"{method}_augmentation_{j}\"] = \"\"\n",
        "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Processing batches\"):\n",
        "            batch_texts = texts[i:i+self.batch_size]\n",
        "            paraphrased_batch_columns= self.transform(method, batch_texts)\n",
        "#             print(len())\n",
        "            for j in range (self.num_return_sequences):\n",
        "                df.loc[i:i + len(paraphrased_batch_columns[j]) - 1, f\"{method}_augmentation_{j}\"] = paraphrased_batch_columns[j]\n",
        "#             paraphrased_texts_french.extend(paraphrased_batch_french)\n",
        "            gc_indicator+=1\n",
        "            if gc_indicator % 10 == 0:\n",
        "                # Save only the processed portion of the DataFrame to avoid excessive file size during early saves\n",
        "                if i + self.batch_size < len(texts):\n",
        "                    df[:i + self.batch_size].to_csv(f\"incremental_augmented_{df_name}_{method}_{gc_indicator // 10}.csv\", index=False)\n",
        "                else:\n",
        "                    df.to_csv(f\"incremental_augmented_{df_name}_{method}_{gc_indicator // 10}.csv\", index=False)\n",
        "                gc.collect()\n",
        "        df.to_csv(f\"augmneted_paraphrased_{df_name}_{method}.csv\", index = False)\n",
        "        return df\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T12:34:32.830657Z",
          "iopub.execute_input": "2024-03-23T12:34:32.830937Z",
          "iopub.status.idle": "2024-03-23T12:34:33.141300Z",
          "shell.execute_reply.started": "2024-03-23T12:34:32.830908Z",
          "shell.execute_reply": "2024-03-23T12:34:33.140330Z"
        },
        "trusted": true,
        "id": "sOqZE6KmNBpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_translator = BackTranslator(batch_size=64)\n",
        "augmented_df = back_translator.augment_dataframe(df, \"back_french_second_run\",'title', \"helsinki\")\n",
        "augmented_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "qG53qgG7NBpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_translator = BackTranslator(batch_size=16)\n",
        "augmented_df = back_translator.augment_dataframe(df, \"back_french_mbart\",'text', \"mbart\")\n",
        "augmented_df"
      ],
      "metadata": {
        "id": "ENPF2eoXOEt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Praphraser Module:**"
      ],
      "metadata": {
        "id": "uJtcEhmQP2Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "class Paraphraser:\n",
        "    def __init__(self, num_return_sequences=5, num_beams=5, max_length=128, batch_size=8):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.num_return_sequences = num_return_sequences\n",
        "        self.num_beams = num_beams\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.models_cache = {}\n",
        "\n",
        "    def _load_model_and_tokenizer(self, model_name):\n",
        "        if model_name not in self.models_cache:\n",
        "            if \"pegasus\" in model_name:\n",
        "                tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "                model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "                print(\"adding pegasus to dict\")\n",
        "                self.models_cache[model_name] = (tokenizer, model)\n",
        "            else:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
        "                self.models_cache[model_name] = (tokenizer, model)\n",
        "                print(f\"adding {model_name} to dict\")\n",
        "        return self.models_cache[model_name]\n",
        "\n",
        "    def transform(self, texts, method):\n",
        "        if method == \"bart\":\n",
        "            return self._bart_paraphrase(texts)\n",
        "        elif method == \"chatgpt_t5\":\n",
        "            return self._chatgpt_t5_paraphrase(texts)\n",
        "        elif method == \"pegasus\":\n",
        "            return self._pegasus_paraphrase(texts)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported paraphrase method: {method}\")\n",
        "\n",
        "    def _bart_paraphrase(self, texts):\n",
        "        tokenizer, model = self._load_model_and_tokenizer('eugenesiow/bart-paraphrase')\n",
        "\n",
        "        batch = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length).to(self.device)\n",
        "        generated = model.generate(batch['input_ids'],\n",
        "                                   num_return_sequences=self.num_return_sequences,\n",
        "                                   num_beams=self.num_beams,\n",
        "                                   max_length=self.max_length,\n",
        "                                   temperature=1.5)\n",
        "        paraphrases = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "\n",
        "        if self.num_return_sequences == 1:\n",
        "            return paraphrases\n",
        "        else:\n",
        "            augmentations_per_txt = [paraphrases[i:i + self.num_return_sequences]\n",
        "                                     for i in range(0, len(paraphrases), self.num_return_sequences)]\n",
        "            # a transpose operation:\n",
        "            augmentations_per_column = [list(group) for group in zip(*augmentations_per_txt)]\n",
        "            return augmentations_per_column\n",
        "\n",
        "    def _pegasus_paraphrase(self, texts):\n",
        "        tokenizer, model = self._load_model_and_tokenizer('tuner007/pegasus_paraphrase')\n",
        "\n",
        "        batch = tokenizer(texts, padding=\"longest\", truncation=True,\n",
        "                          max_length=256, return_tensors=\"pt\").to(self.device)\n",
        "        success = True\n",
        "        try:\n",
        "            generated = model.generate(batch[\"input_ids\"],\n",
        "                                       max_length=256,\n",
        "                                       num_beams=self.num_beams,\n",
        "                                       num_return_sequences=self.num_return_sequences,\n",
        "                                       temperature=1,\n",
        "                                      do_sample = True)\n",
        "            paraphrases = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "        except:\n",
        "            success = False\n",
        "\n",
        "        if self.num_return_sequences == 1:\n",
        "            return paraphrases\n",
        "        else:\n",
        "            if success:\n",
        "                augmentations_per_txt = [paraphrases[i:i + self.num_return_sequences]\n",
        "                                         for i in range(0, len(paraphrases), self.num_return_sequences)]\n",
        "            else:\n",
        "                print(\"not succeed anyway\")\n",
        "                print(batch['input_ids'].size(1))\n",
        "                augmentations_per_txt = []\n",
        "                for text in texts:\n",
        "                    for i in range (self.num_return_sequences):\n",
        "                        augmentations_per_txt.append(text)\n",
        "            # a transpose operation:\n",
        "            augmentations_per_column = [list(group) for group in zip(*augmentations_per_txt)]\n",
        "            return augmentations_per_column\n",
        "\n",
        "    def _chatgpt_t5_paraphrase(self, texts):\n",
        "        tokenizer, model = self._load_model_and_tokenizer(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
        "\n",
        "        paraphrase_prompts = [f\"paraphrase: {text}\" for text in texts]\n",
        "\n",
        "        batch = tokenizer(paraphrase_prompts, return_tensors=\"pt\", padding=\"longest\", truncation=True,\n",
        "                          max_length=self.max_length).to(self.device)\n",
        "        generated = model.generate(\n",
        "            batch['input_ids'],\n",
        "            max_length=self.max_length,\n",
        "            num_beams=self.num_beams,\n",
        "            num_return_sequences=self.num_return_sequences,\n",
        "            temperature=0.7,\n",
        "            num_beam_groups=5,\n",
        "            repetition_penalty=10.0,\n",
        "            diversity_penalty=3.0,\n",
        "            no_repeat_ngram_size=2,\n",
        "        )\n",
        "\n",
        "        paraphrases = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "\n",
        "        if self.num_return_sequences == 1:\n",
        "            return paraphrases\n",
        "        else:\n",
        "            augmentations_per_txt = [paraphrases[i:i + self.num_return_sequences]\n",
        "                                     for i in range(0, len(paraphrases), self.num_return_sequences)]\n",
        "            # a transpose operation:\n",
        "            augmentations_per_column = [list(group) for group in zip(*augmentations_per_txt)]\n",
        "            return augmentations_per_column\n",
        "\n",
        "    def augment_dataframe(self, df, df_name, column_name, method):\n",
        "        texts = df[column_name].tolist()\n",
        "        gc_indicator = 0\n",
        "        for j in range (self.num_return_sequences):\n",
        "            df[f\"{method}_augmentation_{j}\"] = \"\"\n",
        "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Processing batches\"):\n",
        "            batch_texts = texts[i:i+self.batch_size]\n",
        "            paraphrased_batch_columns= self.transform(batch_texts, method)\n",
        "#             print(len())\n",
        "            for j in range (self.num_return_sequences):\n",
        "                df.loc[i:i + len(paraphrased_batch_columns[j]) - 1, f\"{method}_augmentation_{j}\"] = paraphrased_batch_columns[j]\n",
        "#             paraphrased_texts_french.extend(paraphrased_batch_french)\n",
        "            gc_indicator+=1\n",
        "            if gc_indicator % 10 == 0:\n",
        "                # Save only the processed portion of the DataFrame to avoid excessive file size during early saves\n",
        "                if i + self.batch_size < len(texts):\n",
        "                    df[:i + self.batch_size].to_csv(f\"incremental_augmented_{df_name}_{method}_{gc_indicator // 10}.csv\", index=False)\n",
        "                else:\n",
        "                    df.to_csv(f\"incremental_augmented_{df_name}_{method}_{gc_indicator // 10}.csv\", index=False)\n",
        "                gc.collect()\n",
        "        df.to_csv(f\"augmneted_paraphrased_{df_name}_{method}.csv\", index = False)\n",
        "        return df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T09:18:19.429639Z",
          "iopub.execute_input": "2024-03-23T09:18:19.430270Z",
          "iopub.status.idle": "2024-03-23T09:18:19.753664Z",
          "shell.execute_reply.started": "2024-03-23T09:18:19.430234Z",
          "shell.execute_reply": "2024-03-23T09:18:19.752700Z"
        },
        "trusted": true,
        "id": "FFUONi3HNBpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T12:27:34.765479Z",
          "iopub.execute_input": "2024-03-23T12:27:34.766566Z",
          "iopub.status.idle": "2024-03-23T12:27:35.112963Z",
          "shell.execute_reply.started": "2024-03-23T12:27:34.766525Z",
          "shell.execute_reply": "2024-03-23T12:27:35.112062Z"
        },
        "trusted": true,
        "id": "ALc5TcOjNBpP",
        "outputId": "07482421-0ed1-4064-9fa5-967009b2c172"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "804"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paraphraser = Paraphraser(batch_size=32,num_beams=5, max_length=128,num_return_sequences=5  )\n",
        "# paraphraser.augment_dataframe(df, \"pegasus\", \"title\",\"pegasus\")\n",
        "paraphraser.augment_dataframe(df, \"chatgpt_t5\", \"title\",\"chatgpt_t5\")\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-23T09:18:45.847676Z",
          "iopub.execute_input": "2024-03-23T09:18:45.848296Z",
          "iopub.status.idle": "2024-03-23T09:19:48.384240Z",
          "shell.execute_reply.started": "2024-03-23T09:18:45.848269Z",
          "shell.execute_reply": "2024-03-23T09:19:48.382815Z"
        },
        "trusted": true,
        "id": "UEzgZkkGNBpP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_df.to_csv(\"final_T5_gpt.csv\", index = False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jEUiVYV9NBpP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}